{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/harjeet88/llm-course/blob/main/empty_notebook.ipynb",
      "authorship_tag": "ABX9TyP0Jj3hYybq5RQ5nw9Zbnks",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harjeet88/llm-course/blob/main/4_module/1_simple_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the Google Generative AI SDK\n",
        "!pip install -q openai google-generativeai faiss-cpu sentence_transformers"
      ],
      "metadata": {
        "id": "ZZmOBXNFKRwm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import libraries\n",
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "#import google.generativeai as genai\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "A0aQlfNRKa_L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from huggingface_hub import InferenceClient"
      ],
      "metadata": {
        "id": "jsCfBbgCSw_i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WxkB_NqnJfBa"
      },
      "outputs": [],
      "source": [
        "# Step 3\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "GEMINI_API_KEY=GOOGLE_API_KEY\n",
        "gemini_model='gemini-2.5-flash'\n",
        "gemini_api_endpoint=\"https://generativelanguage.googleapis.com/v1beta/openai/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Initialize the model (using Gemini flash)\n",
        "# Configure OpenAI client to use Google's Gemini endpoint\n",
        "gemini = OpenAI(api_key=GOOGLE_API_KEY, base_url=gemini_api_endpoint)"
      ],
      "metadata": {
        "id": "pIB4Syqu24ZV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Hugging Face API token (get a free token from huggingface.co)\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HUGGINGFACE_API_KEY')\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HUGGINGFACE_API_KEY')"
      ],
      "metadata": {
        "id": "iByrHS7-TzwV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"i am harjeet. i work at bla bla company. i am from Bangalore India.\" > \"sample_ai_textbook.txt\""
      ],
      "metadata": {
        "id": "snVSviKcUDE0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and chunk document\n",
        "with open(\"sample_ai_textbook.txt\", \"r\") as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "A4N7vHWcT-dL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple chunking by splitting into paragraphs (approx. 500 chars)\n",
        "chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
        "print(f\"Created {len(chunks)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD0nbKecUYMl",
        "outputId": "c7d53f7e-5ec9-432d-ddc8-1cc0a18888e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 1 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create embeddings\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(chunks, convert_to_numpy=True)\n",
        "dimension = embeddings.shape[1]  # Embedding size (e.g., 384)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut8LNG-4UcTv",
        "outputId": "426325e7-905a-435b-8bce-620473e59ad9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Store embeddings in FAISS\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "print(\"Embeddings stored in FAISS\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uX5L14IUgNn",
        "outputId": "d364b958-d0f0-4351-f990-8a82db13b4fd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings stored in FAISS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrive_chunk(question):\n",
        "  question_embedding = model.encode([question])[0]\n",
        "  D, I = index.search(np.array([question_embedding]), k=1)  # Find top 1 chunk\n",
        "  retrieved_chunk = chunks[I[0][0]]\n",
        "  print(\"Retrieved chunk:\", retrieved_chunk[:100], \"...\")\n",
        "  return retrieved_chunk"
      ],
      "metadata": {
        "id": "aaw2mQUsU6O1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getAnswer(question) :\n",
        "  retrieved_chunk = retrive_chunk(question)\n",
        "  system=\"you are a LLM using RAG to answer user questions. you are impersonating the person mentioned in RAG. so strictly build answers based on context. If you donot know answer, say i dont know  \"\n",
        "  prompt = f\"Context: {retrieved_chunk}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "  answer = gemini.chat.completions.create(\n",
        "      model=gemini_model,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": system},\n",
        "          {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "  ).choices[0].message.content\n",
        "  return answer"
      ],
      "metadata": {
        "id": "icl4yBTRVnFN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Query the system\n",
        "question = \"where do you work?\"\n",
        "answer=getAnswer(question)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_t_AQ9dUldY",
        "outputId": "7e273b53-e4b1-40a1-d477-cbaac414254b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved chunk: i am harjeet. i work at bla bla company. i am from Bangalore India.\n",
            " ...\n",
            "Question: where do you work?\n",
            "Answer: I work at bla bla company.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "38J7VTRFWn1n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}