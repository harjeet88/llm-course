{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/harjeet88/llm-course/blob/main/empty_notebook.ipynb",
      "authorship_tag": "ABX9TyPhg8V2PnNZZI0dCKL3Gfjy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harjeet88/llm-course/blob/main/projects/1_robo_interviewer_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio google-generativeai pydantic"
      ],
      "metadata": {
        "id": "tAANt12gA63l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import sqlite3\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Tuple\n",
        "import uuid\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "cnYp0GLQw5MS"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "fv5PHjAJE8fe"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In-memory state store\n",
        "state_store = {}"
      ],
      "metadata": {
        "id": "nzIG7FGYMfYQ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydantic models for data validation\n",
        "class InterviewConfig(BaseModel):\n",
        "    num_questions: int\n",
        "    technology: str\n",
        "    candidate_id: str\n",
        "    difficulty: str\n",
        "\n",
        "class InterviewResponse(BaseModel):\n",
        "    question: str\n",
        "    answer: str\n",
        "    score: int\n",
        "    feedback: str"
      ],
      "metadata": {
        "id": "C79mITCVAC_B"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Database setup\n",
        "def init_db():\n",
        "    conn = sqlite3.connect('interview.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS interviews\n",
        "                 (interview_id TEXT PRIMARY KEY, candidate_id TEXT, config TEXT, start_time TEXT)''')\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS responses\n",
        "                 (response_id TEXT PRIMARY KEY, interview_id TEXT, question TEXT, answer TEXT, score INTEGER, feedback TEXT)''')\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "TwedKuYtAGlD"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "GEMINI_API_KEY=GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "svWFPIBWAOZq"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini API setup\n",
        "def configure_gemini():\n",
        "    api_key = GEMINI_API_KEY\n",
        "    if not api_key:\n",
        "        raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n",
        "    genai.configure(api_key=api_key)\n",
        "    return genai.GenerativeModel('gemini-2.5-flash')"
      ],
      "metadata": {
        "id": "FAG58cwTAKPt"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean raw response to extract JSON\n",
        "def clean_json_response(raw_text: str) -> str:\n",
        "    cleaned = re.sub(r'^```json\\n|```$', '', raw_text).strip()\n",
        "    cleaned = re.sub(r',\\s*}', '}', cleaned)\n",
        "    cleaned = re.sub(r',\\s*\\]', ']', cleaned)\n",
        "    return cleaned"
      ],
      "metadata": {
        "id": "ibqwHlNbcFIo"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_previous_questions(interview_id: str) -> List[str]:\n",
        "    conn = sqlite3.connect('interview.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT question FROM responses WHERE interview_id = ?\", (interview_id,))\n",
        "    questions = [row[0] for row in c.fetchall()]\n",
        "    conn.close()\n",
        "    return questions"
      ],
      "metadata": {
        "id": "kLWjFhR9Xqme"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_question(model, technology: str, difficulty: str, question_number: int, interview_id: str) -> Tuple[str, str]:\n",
        "    previous_questions = get_previous_questions(interview_id)\n",
        "    previous_questions_str = \"\\n\".join(previous_questions) if previous_questions else \"None\"\n",
        "    prompt = f\"\"\"\n",
        "    Generate a {technology} interview question ({difficulty} level) with a clear answer key. Ensure the question is different from the following previously asked questions:\n",
        "    {previous_questions_str}\n",
        "    Format as JSON: {{'question': '...', 'answer_key': '...'}}\n",
        "    \"\"\"\n",
        "    max_attempts = 3\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            logger.debug(f\"Raw question response (attempt {attempt + 1}): {response.text}\")\n",
        "            cleaned_response = clean_json_response(response.text)\n",
        "            result = json.loads(cleaned_response)\n",
        "            question = result['question']\n",
        "            if question not in previous_questions:\n",
        "                return question, result['answer_key']\n",
        "            logger.debug(f\"Question repeated: {question}, retrying...\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating question (attempt {attempt + 1}): {e}\")\n",
        "            if attempt == max_attempts - 1:\n",
        "                fallback_question = f\"Explain a unique {technology} concept ({difficulty} level) not previously covered.\"\n",
        "                return fallback_question, f\"A clear explanation of a unique {technology} concept.\"\n",
        "    return fallback_question, f\"A clear explanation of a unique {technology} concept.\""
      ],
      "metadata": {
        "id": "GElHGAkJAY7V"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate response using Gemini\n",
        "def evaluate_response(model, question: str, answer: str, answer_key: str) -> Tuple[int, str]:\n",
        "    prompt = f\"\"\"\n",
        "    Evaluate the following candidate response for correctness, clarity, and completeness on a scale of 0-100.\n",
        "    Question: {question}\n",
        "    Candidate Response: {answer}\n",
        "    Reference Answer: {answer_key}\n",
        "    Return JSON: {{'score': int, 'feedback': 'string'}}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        result = json.loads(response.text.strip('```json\\n').strip('```'))\n",
        "        return result['score'], result['feedback']\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error evaluating response: {e}\")\n",
        "        return 50, f\"Evaluation failed: {str(e)}. Default score assigned.\""
      ],
      "metadata": {
        "id": "SjGhlC6CAcNe"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store interview and response data\n",
        "def save_interview(candidate_id: str, config: dict) -> str:\n",
        "    interview_id = str(uuid.uuid4())\n",
        "    conn = sqlite3.connect('interview.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"INSERT INTO interviews (interview_id, candidate_id, config, start_time) VALUES (?, ?, ?, datetime('now'))\",\n",
        "              (interview_id, candidate_id, json.dumps(config)))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    return interview_id\n"
      ],
      "metadata": {
        "id": "q1C8ZWBkAfWg"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_response(interview_id: str, question: str, answer: str, score: int, feedback: str):\n",
        "    response_id = str(uuid.uuid4())\n",
        "    conn = sqlite3.connect('interview.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"INSERT INTO responses (response_id, interview_id, question, answer, score, feedback) VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "              (response_id, interview_id, question, answer, score, feedback))\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "I8uoMqTABCuZ"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get final score\n",
        "def get_final_score(interview_id: str, total_questions: int) -> Tuple[float, List[InterviewResponse]]:\n",
        "    conn = sqlite3.connect('interview.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"SELECT question, answer, score, feedback FROM responses WHERE interview_id = ?\", (interview_id,))\n",
        "    responses = [InterviewResponse(question=row[0], answer=row[1], score=row[2], feedback=row[3]) for row in c.fetchall()]\n",
        "    conn.close()\n",
        "    if responses:\n",
        "        final_score = sum(r.score for r in responses) / total_questions  # Divide by total questions, not answered\n",
        "        return final_score, responses\n",
        "    return 0, []"
      ],
      "metadata": {
        "id": "mS3yvKmqBF-7"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface logic\n",
        "def start_interview(num_questions: int, technology: str, difficulty: str, candidate_id: str):\n",
        "    logger.debug(f\"Starting interview: num_questions={num_questions}, technology={technology}, difficulty={difficulty}, candidate_id={candidate_id}\")\n",
        "    if num_questions < 1 or num_questions > 10:\n",
        "        return \"Number of questions must be between 1 and 10.\", [], \"\", None\n",
        "    if not candidate_id:\n",
        "        return \"Please provide a candidate ID.\", [], \"\", None\n",
        "    if difficulty not in [\"Beginner\", \"Intermediate\", \"Expert\"]:\n",
        "        return \"Invalid difficulty level.\", [], \"\", None\n",
        "\n",
        "    try:\n",
        "        model = configure_gemini()\n",
        "        config = {\"num_questions\": num_questions, \"technology\": technology, \"difficulty\": difficulty}\n",
        "        interview_id = save_interview(candidate_id, config)\n",
        "        question, answer_key = generate_question(model, technology, difficulty, 1, interview_id)\n",
        "        state = {\"interview_id\": interview_id, \"current_question\": 1, \"answer_key\": answer_key, \"model\": model, \"config\": config}\n",
        "        state_store[interview_id] = state\n",
        "        logger.debug(f\"Interview started: {interview_id}, first question: {question}\")\n",
        "        return question, [(question, None)], interview_id, state\n",
        "    except ValueError as e:\n",
        "        logger.error(f\"Error starting interview: {e}\")\n",
        "        return str(e), [], \"\", None"
      ],
      "metadata": {
        "id": "AbukvYuEBIOC"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def submit_response(message: str, history: List[Tuple[str, str]], interview_id: str, state):\n",
        "    logger.debug(f\"Submitting response: message={message}, history={history}, interview_id={interview_id}, state={state}\")\n",
        "\n",
        "    if not state or not isinstance(state, dict):\n",
        "        logger.debug(f\"State missing or invalid, attempting to retrieve from store with interview_id: {interview_id}\")\n",
        "        state = state_store.get(interview_id)\n",
        "        if not state:\n",
        "            logger.error(f\"No state found for interview_id: {interview_id}\")\n",
        "            return \"Interview not started or session expired. Please start a new interview.\", history, interview_id, None\n",
        "\n",
        "    interview_id = state.get(\"interview_id\")\n",
        "    current_question = state.get(\"current_question\")\n",
        "    model = state.get(\"model\")\n",
        "    config = state.get(\"config\")\n",
        "    answer_key = state.get(\"answer_key\")\n",
        "\n",
        "    if not all([interview_id, current_question, model, config, answer_key]):\n",
        "        logger.error(f\"Invalid state contents: {state}\")\n",
        "        return \"Invalid interview state. Please start a new interview.\", history, interview_id, None\n",
        "\n",
        "    score, feedback = evaluate_response(model, history[-1][0], message, answer_key)\n",
        "    save_response(interview_id, history[-1][0], message, score, feedback)\n",
        "    history[-1] = (history[-1][0], message)\n",
        "\n",
        "    if current_question < config[\"num_questions\"]:\n",
        "        next_question, next_answer_key = generate_question(model, config[\"technology\"], config[\"difficulty\"], current_question + 1, interview_id)\n",
        "        state[\"current_question\"] = current_question + 1\n",
        "        state[\"answer_key\"] = next_answer_key\n",
        "        state_store[interview_id] = state\n",
        "        history.append((next_question, None))\n",
        "        logger.debug(f\"Next question generated: {next_question}\")\n",
        "        return f\"Answer submitted. Please answer the next question.\", history, interview_id, state\n",
        "    else:\n",
        "        final_score, responses = get_final_score(interview_id, config[\"num_questions\"])\n",
        "        feedback_summary = \"\\n\".join([f\"Q: {r.question}\\nA: {r.answer}\\nScore: {r.score}\\nFeedback: {r.feedback}\" for r in responses])\n",
        "        logger.debug(f\"Interview completed: final_score={final_score}\")\n",
        "        state_store.pop(interview_id, None)\n",
        "        return f\"Interview completed! Final Score: {final_score:.2f}/100\\n\\nSummary:\\n{feedback_summary}\", history, \"\", None\n"
      ],
      "metadata": {
        "id": "O9pUJmA2BKww"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def end_interview(interview_id: str, history: List[Tuple[str, str]], state):\n",
        "    logger.debug(f\"Ending interview: interview_id={interview_id}, state={state}\")\n",
        "\n",
        "    # Retrieve state from store if not provided\n",
        "    if not state or not isinstance(state, dict):\n",
        "        logger.debug(f\"State missing or invalid, attempting to retrieve from store with interview_id: {interview_id}\")\n",
        "        state = state_store.get(interview_id)\n",
        "        if not state:\n",
        "            logger.error(f\"No state found for interview_id: {interview_id}\")\n",
        "            return \"Interview not started or session expired.\", history, interview_id, None\n",
        "\n",
        "    # Validate state contents\n",
        "    interview_id = state.get(\"interview_id\")\n",
        "    config = state.get(\"config\")\n",
        "\n",
        "    if not all([interview_id, config]):\n",
        "        logger.error(f\"Invalid state contents: {state}\")\n",
        "        return \"Invalid interview state.\", history, interview_id, None\n",
        "\n",
        "    # Calculate final score based on all questions\n",
        "    final_score, responses = get_final_score(interview_id, config[\"num_questions\"])\n",
        "    feedback_summary = \"\\n\".join([f\"Q: {r.question}\\nA: {r.answer}\\nScore: {r.score}\\nFeedback: {r.feedback}\" for r in responses])\n",
        "    logger.debug(f\"Interview ended early: final_score={final_score}\")\n",
        "    # Clear state from store\n",
        "    state_store.pop(interview_id, None)\n",
        "    return f\"Interview ended early! Final Score: {final_score:.2f}/100\\n\\nSummary:\\n{feedback_summary}\", history, \"\", None"
      ],
      "metadata": {
        "id": "PHOwTDGPZAy2"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize database\n",
        "init_db()"
      ],
      "metadata": {
        "id": "uTh_PDioBNyj"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Robo Interviewer System\")\n",
        "    candidate_id = gr.Textbox(label=\"Candidate ID\")\n",
        "    num_questions = gr.Slider(minimum=1, maximum=10, step=1, label=\"Number of Questions\", value=5)\n",
        "    technology = gr.Dropdown(choices=[\"Python\", \"Java\", \"DevOps\", \"JavaScript\"], label=\"Technology\", value=\"Python\")\n",
        "    difficulty = gr.Dropdown(choices=[\"Beginner\", \"Intermediate\", \"Expert\"], label=\"Difficulty Level\", value=\"Intermediate\")\n",
        "    interview_id = gr.Textbox(label=\"Interview ID\", visible=False)  # Hidden field for interview_id\n",
        "    start_btn = gr.Button(\"Start Interview\")\n",
        "    end_btn = gr.Button(\"End Interview\")\n",
        "    chatbot = gr.Chatbot(label=\"Interview Chat\")\n",
        "    msg = gr.Textbox(label=\"Your Answer\")\n",
        "    output = gr.Textbox(label=\"Feedback and Score\")\n",
        "\n",
        "    start_btn.click(\n",
        "        fn=start_interview,\n",
        "        inputs=[num_questions, technology, difficulty, candidate_id],\n",
        "        outputs=[output, chatbot, interview_id, gr.State()]\n",
        "    )\n",
        "    msg.submit(\n",
        "        fn=submit_response,\n",
        "        inputs=[msg, chatbot, interview_id, gr.State()],\n",
        "        outputs=[output, chatbot, interview_id, gr.State()]\n",
        "    )\n",
        "    end_btn.click(\n",
        "        fn=end_interview,\n",
        "        inputs=[interview_id, chatbot, gr.State()],\n",
        "        outputs=[output, chatbot, interview_id, gr.State()]\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqT1U6WqBQA7",
        "outputId": "26013b3b-066f-4a4d-ca0d-9e623f82fbc9"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-112-1105992436.py:11: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Interview Chat\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "SbTT9clgBTaV",
        "outputId": "e924874a-536b-42a4-f9e1-43dbb0750c10"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://bd225d3928e7b23a26.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bd225d3928e7b23a26.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error evaluating response: Expecting value: line 1 column 1 (char 0)\n",
            "ERROR:__main__:Error evaluating response: Expecting value: line 1 column 1 (char 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://bd225d3928e7b23a26.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c_D5XMOsBY1n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}